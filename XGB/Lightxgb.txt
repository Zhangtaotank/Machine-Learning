01: Introduction to Gradient Boosting
  
    Boosting: In selecting the samples for building the models, boosting is a good way to adjust the weights of the samples, it increases the probility of choosing the weak samples to make the learner better. https://www.youtube.com/watch?v=GM3CDQfQ4sw
  
  The Gradient Boosting algorithm involves three elements:
    1. A loss function to be optimized, such as cross entropy for classification or mean squared error for regression problems.
    2. A weak learner to make predictions, such as a greedily constructed decision tree.
    3. An additive model, used to add weak learners to minimize the loss function.
    
    AdaBoost and related algorithms were recast in a statistical framework and became known as Gradient Boosting Machines. The statistical framework cast boosting as a numerical optimization problem where the objective is to minimize the loss of the model by adding weak learners using a gradient descent like procedure, hence the name.
  
    New weak learners are added to the model in an effort to correct the residual errors of all previous trees. The result is a powerful predictive modeling algorithm, perhaps more powerful than random forest.
    
02: Introduction to XGBoost

    XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. XGBoost stands for eXtreme Gradient Boosting. In addition to supporting all key variations of the technique, the real interest is the speed provided by the careful engineering of the implementation, including:

      1.Parallelization of tree construction using all of your CPU cores during training.
      2. Distributed Computing for training very large models using a cluster of machines.
      3.Out-of-Core Computing for very large datasets that donâ€™t fit into memory.
      4.Cache Optimization of data structures and algorithms to make best use of hardware.

03 Configure Gradient Boosting
    A number of configuration heuristics were published in the original gradient boosting papers.
      1.Learning rate or shrinkage (learning_rate in XGBoost) should be set to 0.1 or lower, and smaller values will require the addition of more trees.
      2.The depth of trees (tree_depth in XGBoost) should be configured in the range of 2-to-8, where not much benefit is seen with deeper trees.
      3.Row sampling (subsample in XGBoost) should be configured in the range of 30% to 80% of the training dataset, and compared to a value of 100% for no sampling.
      
      A good general configuration strategy is as follows:

      1. Run the default configuration and review plots of the learning curves on the training and validation datasets.
      2. If the system is overlearning, decrease the learning rate and/or increase the number of trees.
      3.If the system is underlearning, speed the learning up to be more aggressive by increasing the learning rate and/or decreasing the number of trees.
      
       I can set the number of trees to a target value such as 100 or 1000, then tune the learning rate to find the best model. This is an efficient strategy for quickly finding a good model.
       

04 XGBoost Hyperparameter Tuning
    The scikit-learn framework provides the capability to search combinations of parameters. This capability is provided in the GridSearchCV class and can be used to discover the best way to configure the model for top performance on your problem.
    
    https://machinelearningmastery.com/xgboost-python-mini-course/

      
      
      
      

To be updated if needed!
    
    
  
